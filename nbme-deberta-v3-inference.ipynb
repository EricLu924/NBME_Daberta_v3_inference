{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4711f9a2",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-30T10:07:09.398646Z",
     "iopub.status.busy": "2025-05-30T10:07:09.398259Z",
     "iopub.status.idle": "2025-05-30T10:07:31.272239Z",
     "shell.execute_reply": "2025-05-30T10:07:31.271365Z"
    },
    "papermill": {
     "duration": 21.880275,
     "end_time": "2025-05-30T10:07:31.274157",
     "exception": false,
     "start_time": "2025-05-30T10:07:09.393882",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f97ea2af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T10:07:31.281323Z",
     "iopub.status.busy": "2025-05-30T10:07:31.280807Z",
     "iopub.status.idle": "2025-05-30T10:07:31.286333Z",
     "shell.execute_reply": "2025-05-30T10:07:31.285139Z"
    },
    "papermill": {
     "duration": 0.010916,
     "end_time": "2025-05-30T10:07:31.288114",
     "exception": false,
     "start_time": "2025-05-30T10:07:31.277198",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# CFG\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    debug = False\n",
    "    num_workers = 0\n",
    "    model = \"/kaggle/input/deberta-v3/deberta_base_cache\"\n",
    "    batch_size = 8\n",
    "    max_len = 512\n",
    "    seed = 42\n",
    "    n_fold = 5\n",
    "    trn_fold = [0, 1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15e509fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T10:07:31.294979Z",
     "iopub.status.busy": "2025-05-30T10:07:31.294198Z",
     "iopub.status.idle": "2025-05-30T10:07:31.310545Z",
     "shell.execute_reply": "2025-05-30T10:07:31.309389Z"
    },
    "papermill": {
     "duration": 0.021553,
     "end_time": "2025-05-30T10:07:31.312372",
     "exception": false,
     "start_time": "2025-05-30T10:07:31.290819",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Utils\n",
    "# ====================================================\n",
    "def get_logger(filename='inference'):\n",
    "    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = get_logger()\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything(seed=CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3ef7d65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T10:07:31.319585Z",
     "iopub.status.busy": "2025-05-30T10:07:31.318390Z",
     "iopub.status.idle": "2025-05-30T10:07:31.327461Z",
     "shell.execute_reply": "2025-05-30T10:07:31.326096Z"
    },
    "papermill": {
     "duration": 0.01432,
     "end_time": "2025-05-30T10:07:31.329313",
     "exception": false,
     "start_time": "2025-05-30T10:07:31.314993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Model\n",
    "# ====================================================\n",
    "class DebertaForTokenBinary(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "        self.backbone = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(self.backbone.config.hidden_size, 1)\n",
    "        \n",
    "    def forward(self, **batch):\n",
    "        labels = batch.pop(\"labels\", None)\n",
    "        out = self.backbone(**batch)\n",
    "        logits = self.classifier(self.dropout(out.last_hidden_state)).squeeze(-1)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fct(logits, labels)\n",
    "        \n",
    "        return {\"loss\": loss, \"logits\": logits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4f0c845",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T10:07:31.336528Z",
     "iopub.status.busy": "2025-05-30T10:07:31.335819Z",
     "iopub.status.idle": "2025-05-30T10:07:31.344587Z",
     "shell.execute_reply": "2025-05-30T10:07:31.343719Z"
    },
    "papermill": {
     "duration": 0.014271,
     "end_time": "2025-05-30T10:07:31.346266",
     "exception": false,
     "start_time": "2025-05-30T10:07:31.331995",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Dataset\n",
    "# ====================================================\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, tokenizer, df):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.feature_texts = df['feature_text'].values\n",
    "        self.pn_historys = df['pn_history'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.feature_texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        inputs = self.tokenizer(\n",
    "            str(self.feature_texts[item]),\n",
    "            str(self.pn_historys[item]), \n",
    "            add_special_tokens=True,\n",
    "            max_length=CFG.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=False,\n",
    "            truncation=\"only_second\"\n",
    "        )\n",
    "        return {k: torch.tensor(v, dtype=torch.long) for k, v in inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a82309b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T10:07:31.352980Z",
     "iopub.status.busy": "2025-05-30T10:07:31.352637Z",
     "iopub.status.idle": "2025-05-30T10:07:31.373278Z",
     "shell.execute_reply": "2025-05-30T10:07:31.372117Z"
    },
    "papermill": {
     "duration": 0.026198,
     "end_time": "2025-05-30T10:07:31.375144",
     "exception": false,
     "start_time": "2025-05-30T10:07:31.348946",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Helper functions\n",
    "# ====================================================\n",
    "def inference_fn(test_loader, model, device):\n",
    "    preds = []\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    tk0 = tqdm(test_loader, total=len(test_loader))\n",
    "    \n",
    "    for step, inputs in enumerate(tk0):\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            y_preds = outputs[\"logits\"].sigmoid()\n",
    "        preds.append(y_preds.to('cpu').numpy())\n",
    "    \n",
    "    predictions = np.concatenate(preds)\n",
    "    return predictions\n",
    "\n",
    "def get_char_probs(texts, predictions, tokenizer):\n",
    "    results = [np.zeros(len(t)) for t in texts]\n",
    "    for i, (text, prediction) in enumerate(zip(texts, predictions)):\n",
    "        encoded = tokenizer(\n",
    "            text, \n",
    "            add_special_tokens=True,\n",
    "            max_length=CFG.max_len,\n",
    "            padding=\"max_length\", \n",
    "            return_offsets_mapping=True\n",
    "        )\n",
    "        for idx, (offset_mapping, pred) in enumerate(zip(encoded['offset_mapping'], prediction)):\n",
    "            start = offset_mapping[0]\n",
    "            end = offset_mapping[1]\n",
    "            if start < len(results[i]) and end <= len(results[i]):\n",
    "                results[i][start:end] = pred\n",
    "    return results\n",
    "\n",
    "def get_predictions_from_char_probs(char_probs, threshold=0.5):\n",
    "    predictions = []\n",
    "    for char_prob in char_probs:\n",
    "        prediction = []\n",
    "        inside = False\n",
    "        start_idx = 0\n",
    "        \n",
    "        for i, prob in enumerate(char_prob):\n",
    "            if prob >= threshold and not inside:\n",
    "                start_idx = i\n",
    "                inside = True\n",
    "            elif prob < threshold and inside:\n",
    "                prediction.append(f\"{start_idx} {i}\")\n",
    "                inside = False\n",
    "                \n",
    "        # Handle case where span continues to the end\n",
    "        if inside:\n",
    "            prediction.append(f\"{start_idx} {len(char_prob)}\")\n",
    "            \n",
    "        predictions.append(\" \".join(prediction))\n",
    "    return predictions\n",
    "\n",
    "def find_optimal_threshold(char_probs, ground_truth_locations=None):\n",
    "    \"\"\"\n",
    "    Â∞ãÊâæÊúÄ‰Ω≥ÈñæÂÄº - Â¶ÇÊûúÊ≤íÊúâground truthÔºåÂ∞±Ëº∏Âá∫‰∏çÂêåÈñæÂÄºÁöÑÁµêÊûú‰æõÂèÉËÄÉ\n",
    "    \"\"\"\n",
    "    thresholds = np.arange(0.45, 0.56, 0.01)\n",
    "    best_threshold = 0.5\n",
    "    best_score = 0.0\n",
    "    \n",
    "    LOGGER.info(\"=== Threshold Search ===\")\n",
    "    \n",
    "    for th in thresholds:\n",
    "        predictions = get_predictions_from_char_probs(char_probs, threshold=th)\n",
    "        \n",
    "        if ground_truth_locations is not None:\n",
    "            # Â¶ÇÊûúÊúâground truthÔºåË®àÁÆóÂØ¶ÈöõF1ÂàÜÊï∏\n",
    "            score = compute_f1_score(ground_truth_locations, predictions)\n",
    "        else:\n",
    "            # Â¶ÇÊûúÊ≤íÊúâground truthÔºå‰ΩøÁî®È†êÊ∏¨Êï∏Èáè‰ΩúÁÇ∫ÂèÉËÄÉÊåáÊ®ô\n",
    "            total_predictions = sum(1 for pred in predictions if pred.strip())\n",
    "            avg_prediction_length = np.mean([len(pred) for pred in predictions if pred.strip()])\n",
    "            score = total_predictions / len(predictions) * 0.5 + min(avg_prediction_length / 20, 0.5)\n",
    "        \n",
    "        LOGGER.info(f\"th: {th:.2f}  score: {score:.5f}\")\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_threshold = th\n",
    "    \n",
    "    LOGGER.info(f\"best_th: {best_threshold:.2f}  score: {best_score:.5f}\")\n",
    "    LOGGER.info(\"=\" * 30)\n",
    "    \n",
    "    return best_threshold\n",
    "\n",
    "def compute_f1_score(ground_truth, predictions):\n",
    "    \"\"\"\n",
    "    Ë®àÁÆóF1ÂàÜÊï∏ (Â¶ÇÊûúÊúâground truthÁöÑË©±)\n",
    "    \"\"\"\n",
    "    # ÈÄôÊòØ‰∏ÄÂÄãÁ∞°ÂåñÁöÑF1Ë®àÁÆóÔºåÂØ¶ÈöõÊØîË≥Ω‰∏≠ÊúÉÊõ¥Ë§áÈõú\n",
    "    tp = fp = fn = 0\n",
    "    for gt, pred in zip(ground_truth, predictions):\n",
    "        gt_set = set()\n",
    "        pred_set = set()\n",
    "        \n",
    "        # Ëß£Êûêground truth spans\n",
    "        if gt and gt.strip():\n",
    "            for span in gt.split(';'):\n",
    "                if span.strip():\n",
    "                    try:\n",
    "                        start, end = map(int, span.strip().split())\n",
    "                        gt_set.update(range(start, end))\n",
    "                    except:\n",
    "                        pass\n",
    "        \n",
    "        # Ëß£Êûêprediction spans  \n",
    "        if pred and pred.strip():\n",
    "            for span in pred.split(';'):\n",
    "                if span.strip():\n",
    "                    try:\n",
    "                        start, end = map(int, span.strip().split())\n",
    "                        pred_set.update(range(start, end))\n",
    "                    except:\n",
    "                        pass\n",
    "        \n",
    "        tp += len(gt_set & pred_set)\n",
    "        fp += len(pred_set - gt_set)\n",
    "        fn += len(gt_set - pred_set)\n",
    "    \n",
    "    f1 = 2 * tp / (2 * tp + fp + fn + 1e-8)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a04ac1e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T10:07:31.382483Z",
     "iopub.status.busy": "2025-05-30T10:07:31.382147Z",
     "iopub.status.idle": "2025-05-30T10:07:33.051511Z",
     "shell.execute_reply": "2025-05-30T10:07:33.050293Z"
    },
    "papermill": {
     "duration": 1.675253,
     "end_time": "2025-05-30T10:07:33.053171",
     "exception": true,
     "start_time": "2025-05-30T10:07:31.377918",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading test data...\n",
      "Test data shape: (5, 6)\n",
      "Loading tokenizer...\n",
      "Loading tokenizer from local path: /kaggle/input/deberta-v3/deberta_base_cache\n",
      "‚úÖ Tokenizer loaded successfully\n",
      "Starting inference...\n",
      "Processing fold 0...\n",
      "‚ùå Model file not found: /kaggle/input/nbme-deberta-v3/nbme_ckpt/fold0.pt\n",
      "Processing fold 1...\n",
      "‚ùå Model file not found: /kaggle/input/nbme-deberta-v3/nbme_ckpt/fold1.pt\n",
      "Processing fold 2...\n",
      "‚ùå Model file not found: /kaggle/input/nbme-deberta-v3/nbme_ckpt/fold2.pt\n",
      "Processing fold 3...\n",
      "‚ùå Model file not found: /kaggle/input/nbme-deberta-v3/nbme_ckpt/fold3.pt\n",
      "Processing fold 4...\n",
      "‚ùå Model file not found: /kaggle/input/nbme-deberta-v3/nbme_ckpt/fold4.pt\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "‚ùå No models were successfully loaded!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13/412120855.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_13/412120855.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"‚ùå No models were successfully loaded!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;31m# ====================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: ‚ùå No models were successfully loaded!"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# main\n",
    "# ====================================================\n",
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    LOGGER.info(f\"Using device: {device}\")\n",
    "    \n",
    "    # ====================================================\n",
    "    # Load test data\n",
    "    # ====================================================\n",
    "    LOGGER.info(\"Loading test data...\")\n",
    "    test = pd.read_csv('/kaggle/input/nbme-score-clinical-patient-notes/test.csv')\n",
    "    features = pd.read_csv('/kaggle/input/nbme-score-clinical-patient-notes/features.csv')\n",
    "    patient_notes = pd.read_csv('/kaggle/input/nbme-score-clinical-patient-notes/patient_notes.csv')\n",
    "    \n",
    "    test = test.merge(features, on=['feature_num', 'case_num'], how='left')\n",
    "    test = test.merge(patient_notes, on=['pn_num', 'case_num'], how='left')\n",
    "    LOGGER.info(f\"Test data shape: {test.shape}\")\n",
    "\n",
    "    # ====================================================\n",
    "    # Load tokenizer\n",
    "    # ====================================================\n",
    "    LOGGER.info(\"Loading tokenizer...\")\n",
    "    try:\n",
    "        if os.path.exists(CFG.model):\n",
    "            LOGGER.info(f\"Loading tokenizer from local path: {CFG.model}\")\n",
    "            tokenizer = AutoTokenizer.from_pretrained(CFG.model)\n",
    "        else:\n",
    "            LOGGER.info(\"Local path not found, using online model...\")\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
    "            CFG.model = \"microsoft/deberta-base\"\n",
    "        LOGGER.info(\"‚úÖ Tokenizer loaded successfully\")\n",
    "    except Exception as e:\n",
    "        LOGGER.error(f\"‚ùå Failed to load tokenizer: {e}\")\n",
    "        raise\n",
    "\n",
    "    # ====================================================\n",
    "    # Create dataset and dataloader\n",
    "    # ====================================================\n",
    "    test_dataset = TestDataset(tokenizer, test)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=CFG.num_workers,\n",
    "        pin_memory=True, \n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    # ====================================================\n",
    "    # Model inference\n",
    "    # ====================================================\n",
    "    predictions = []\n",
    "    LOGGER.info(\"Starting inference...\")\n",
    "    \n",
    "    for fold in CFG.trn_fold:\n",
    "        LOGGER.info(f\"Processing fold {fold}...\")\n",
    "        \n",
    "        try:\n",
    "            model_path = f'/kaggle/input/nbme-deberta-v3/nbme_ckpt/fold{fold}.pt'\n",
    "            if not os.path.exists(model_path):\n",
    "                LOGGER.error(f\"‚ùå Model file not found: {model_path}\")\n",
    "                continue\n",
    "                \n",
    "            # ËºâÂÖ•Ê®°Âûã\n",
    "            model = DebertaForTokenBinary(CFG.model)\n",
    "            checkpoint = torch.load(model_path, map_location='cpu')\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            LOGGER.info(f\"‚úÖ Model loaded for fold {fold}\")\n",
    "            \n",
    "            # Êé®Ë´ñ\n",
    "            prediction = inference_fn(test_loader, model, device)\n",
    "            predictions.append(prediction)\n",
    "            LOGGER.info(f\"‚úÖ Fold {fold} inference completed\")\n",
    "            \n",
    "            del model, checkpoint, prediction\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        except Exception as e:\n",
    "            LOGGER.error(f\"‚ùå Error in fold {fold}: {e}\")\n",
    "            import traceback\n",
    "            LOGGER.error(traceback.format_exc())\n",
    "            continue\n",
    "\n",
    "    if len(predictions) == 0:\n",
    "        raise RuntimeError(\"‚ùå No models were successfully loaded!\")\n",
    "\n",
    "    # ====================================================\n",
    "    # Ensemble and post-processing\n",
    "    # ====================================================\n",
    "    LOGGER.info(\"Averaging predictions...\")\n",
    "    predictions = np.mean(predictions, axis=0)\n",
    "\n",
    "    LOGGER.info(\"Post processing...\")\n",
    "    char_probs = get_char_probs(test['pn_history'].values, predictions, tokenizer)\n",
    "    \n",
    "    # üî• Êñ∞Â¢ûÔºöÈñæÂÄºÊêúÁ¥¢\n",
    "    best_threshold = find_optimal_threshold(char_probs, ground_truth_locations=None)\n",
    "    \n",
    "    predictions = get_predictions_from_char_probs(char_probs, threshold=best_threshold)\n",
    "\n",
    "    # ====================================================\n",
    "    # Create submission\n",
    "    # ====================================================\n",
    "    LOGGER.info(\"Creating submission...\")\n",
    "    test['location'] = predictions\n",
    "    submission = test[['id', 'location']].copy()\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "    LOGGER.info(\"‚úÖ submission.csv saved\")\n",
    "    LOGGER.info(f\"Submission shape: {submission.shape}\")\n",
    "    print(submission.head())\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 3075283,
     "isSourceIdPinned": false,
     "sourceId": 33607,
     "sourceType": "competition"
    },
    {
     "datasetId": 7552460,
     "sourceId": 12005462,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 25568107,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 242336254,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 32.276098,
   "end_time": "2025-05-30T10:07:36.331316",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-30T10:07:04.055218",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

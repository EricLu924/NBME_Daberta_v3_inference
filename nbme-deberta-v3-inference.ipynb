{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4711f9a2",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-30T10:07:09.398646Z",
     "iopub.status.busy": "2025-05-30T10:07:09.398259Z",
     "iopub.status.idle": "2025-05-30T10:07:31.272239Z",
     "shell.execute_reply": "2025-05-30T10:07:31.271365Z"
    },
    "papermill": {
     "duration": 21.880275,
     "end_time": "2025-05-30T10:07:31.274157",
     "exception": false,
     "start_time": "2025-05-30T10:07:09.393882",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f97ea2af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T10:07:31.281323Z",
     "iopub.status.busy": "2025-05-30T10:07:31.280807Z",
     "iopub.status.idle": "2025-05-30T10:07:31.286333Z",
     "shell.execute_reply": "2025-05-30T10:07:31.285139Z"
    },
    "papermill": {
     "duration": 0.010916,
     "end_time": "2025-05-30T10:07:31.288114",
     "exception": false,
     "start_time": "2025-05-30T10:07:31.277198",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# CFG\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    debug = False\n",
    "    num_workers = 0\n",
    "    model = \"/kaggle/input/deberta-v3/deberta_base_cache\"\n",
    "    batch_size = 8\n",
    "    max_len = 512\n",
    "    seed = 42\n",
    "    n_fold = 5\n",
    "    trn_fold = [0, 1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15e509fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T10:07:31.294979Z",
     "iopub.status.busy": "2025-05-30T10:07:31.294198Z",
     "iopub.status.idle": "2025-05-30T10:07:31.310545Z",
     "shell.execute_reply": "2025-05-30T10:07:31.309389Z"
    },
    "papermill": {
     "duration": 0.021553,
     "end_time": "2025-05-30T10:07:31.312372",
     "exception": false,
     "start_time": "2025-05-30T10:07:31.290819",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Utils\n",
    "# ====================================================\n",
    "def get_logger(filename='inference'):\n",
    "    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = get_logger()\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything(seed=CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3ef7d65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T10:07:31.319585Z",
     "iopub.status.busy": "2025-05-30T10:07:31.318390Z",
     "iopub.status.idle": "2025-05-30T10:07:31.327461Z",
     "shell.execute_reply": "2025-05-30T10:07:31.326096Z"
    },
    "papermill": {
     "duration": 0.01432,
     "end_time": "2025-05-30T10:07:31.329313",
     "exception": false,
     "start_time": "2025-05-30T10:07:31.314993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Model\n",
    "# ====================================================\n",
    "class DebertaForTokenBinary(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "        self.backbone = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(self.backbone.config.hidden_size, 1)\n",
    "        \n",
    "    def forward(self, **batch):\n",
    "        labels = batch.pop(\"labels\", None)\n",
    "        out = self.backbone(**batch)\n",
    "        logits = self.classifier(self.dropout(out.last_hidden_state)).squeeze(-1)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fct(logits, labels)\n",
    "        \n",
    "        return {\"loss\": loss, \"logits\": logits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4f0c845",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T10:07:31.336528Z",
     "iopub.status.busy": "2025-05-30T10:07:31.335819Z",
     "iopub.status.idle": "2025-05-30T10:07:31.344587Z",
     "shell.execute_reply": "2025-05-30T10:07:31.343719Z"
    },
    "papermill": {
     "duration": 0.014271,
     "end_time": "2025-05-30T10:07:31.346266",
     "exception": false,
     "start_time": "2025-05-30T10:07:31.331995",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Dataset\n",
    "# ====================================================\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, tokenizer, df):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.feature_texts = df['feature_text'].values\n",
    "        self.pn_historys = df['pn_history'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.feature_texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        inputs = self.tokenizer(\n",
    "            str(self.feature_texts[item]),\n",
    "            str(self.pn_historys[item]), \n",
    "            add_special_tokens=True,\n",
    "            max_length=CFG.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=False,\n",
    "            truncation=\"only_second\"\n",
    "        )\n",
    "        return {k: torch.tensor(v, dtype=torch.long) for k, v in inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a82309b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T10:07:31.352980Z",
     "iopub.status.busy": "2025-05-30T10:07:31.352637Z",
     "iopub.status.idle": "2025-05-30T10:07:31.373278Z",
     "shell.execute_reply": "2025-05-30T10:07:31.372117Z"
    },
    "papermill": {
     "duration": 0.026198,
     "end_time": "2025-05-30T10:07:31.375144",
     "exception": false,
     "start_time": "2025-05-30T10:07:31.348946",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Helper functions\n",
    "# ====================================================\n",
    "def inference_fn(test_loader, model, device):\n",
    "    preds = []\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    tk0 = tqdm(test_loader, total=len(test_loader))\n",
    "    \n",
    "    for step, inputs in enumerate(tk0):\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            y_preds = outputs[\"logits\"].sigmoid()\n",
    "        preds.append(y_preds.to('cpu').numpy())\n",
    "    \n",
    "    predictions = np.concatenate(preds)\n",
    "    return predictions\n",
    "\n",
    "def get_char_probs(texts, predictions, tokenizer):\n",
    "    results = [np.zeros(len(t)) for t in texts]\n",
    "    for i, (text, prediction) in enumerate(zip(texts, predictions)):\n",
    "        encoded = tokenizer(\n",
    "            text, \n",
    "            add_special_tokens=True,\n",
    "            max_length=CFG.max_len,\n",
    "            padding=\"max_length\", \n",
    "            return_offsets_mapping=True\n",
    "        )\n",
    "        for idx, (offset_mapping, pred) in enumerate(zip(encoded['offset_mapping'], prediction)):\n",
    "            start = offset_mapping[0]\n",
    "            end = offset_mapping[1]\n",
    "            if start < len(results[i]) and end <= len(results[i]):\n",
    "                results[i][start:end] = pred\n",
    "    return results\n",
    "\n",
    "def get_predictions_from_char_probs(char_probs, threshold=0.5):\n",
    "    predictions = []\n",
    "    for char_prob in char_probs:\n",
    "        prediction = []\n",
    "        inside = False\n",
    "        start_idx = 0\n",
    "        \n",
    "        for i, prob in enumerate(char_prob):\n",
    "            if prob >= threshold and not inside:\n",
    "                start_idx = i\n",
    "                inside = True\n",
    "            elif prob < threshold and inside:\n",
    "                prediction.append(f\"{start_idx} {i}\")\n",
    "                inside = False\n",
    "                \n",
    "        # Handle case where span continues to the end\n",
    "        if inside:\n",
    "            prediction.append(f\"{start_idx} {len(char_prob)}\")\n",
    "            \n",
    "        predictions.append(\" \".join(prediction))\n",
    "    return predictions\n",
    "\n",
    "def find_optimal_threshold(char_probs, ground_truth_locations=None):\n",
    "    \"\"\"\n",
    "    尋找最佳閾值 - 如果沒有ground truth，就輸出不同閾值的結果供參考\n",
    "    \"\"\"\n",
    "    thresholds = np.arange(0.45, 0.56, 0.01)\n",
    "    best_threshold = 0.5\n",
    "    best_score = 0.0\n",
    "    \n",
    "    LOGGER.info(\"=== Threshold Search ===\")\n",
    "    \n",
    "    for th in thresholds:\n",
    "        predictions = get_predictions_from_char_probs(char_probs, threshold=th)\n",
    "        \n",
    "        if ground_truth_locations is not None:\n",
    "            # 如果有ground truth，計算實際F1分數\n",
    "            score = compute_f1_score(ground_truth_locations, predictions)\n",
    "        else:\n",
    "            # 如果沒有ground truth，使用預測數量作為參考指標\n",
    "            total_predictions = sum(1 for pred in predictions if pred.strip())\n",
    "            avg_prediction_length = np.mean([len(pred) for pred in predictions if pred.strip()])\n",
    "            score = total_predictions / len(predictions) * 0.5 + min(avg_prediction_length / 20, 0.5)\n",
    "        \n",
    "        LOGGER.info(f\"th: {th:.2f}  score: {score:.5f}\")\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_threshold = th\n",
    "    \n",
    "    LOGGER.info(f\"best_th: {best_threshold:.2f}  score: {best_score:.5f}\")\n",
    "    LOGGER.info(\"=\" * 30)\n",
    "    \n",
    "    return best_threshold\n",
    "\n",
    "def compute_f1_score(ground_truth, predictions):\n",
    "    \"\"\"\n",
    "    計算F1分數 (如果有ground truth的話)\n",
    "    \"\"\"\n",
    "    # 這是一個簡化的F1計算，實際比賽中會更複雜\n",
    "    tp = fp = fn = 0\n",
    "    for gt, pred in zip(ground_truth, predictions):\n",
    "        gt_set = set()\n",
    "        pred_set = set()\n",
    "        \n",
    "        # 解析ground truth spans\n",
    "        if gt and gt.strip():\n",
    "            for span in gt.split(';'):\n",
    "                if span.strip():\n",
    "                    try:\n",
    "                        start, end = map(int, span.strip().split())\n",
    "                        gt_set.update(range(start, end))\n",
    "                    except:\n",
    "                        pass\n",
    "        \n",
    "        # 解析prediction spans  \n",
    "        if pred and pred.strip():\n",
    "            for span in pred.split(';'):\n",
    "                if span.strip():\n",
    "                    try:\n",
    "                        start, end = map(int, span.strip().split())\n",
    "                        pred_set.update(range(start, end))\n",
    "                    except:\n",
    "                        pass\n",
    "        \n",
    "        tp += len(gt_set & pred_set)\n",
    "        fp += len(pred_set - gt_set)\n",
    "        fn += len(gt_set - pred_set)\n",
    "    \n",
    "    f1 = 2 * tp / (2 * tp + fp + fn + 1e-8)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a04ac1e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T10:07:31.382483Z",
     "iopub.status.busy": "2025-05-30T10:07:31.382147Z",
     "iopub.status.idle": "2025-05-30T10:07:33.051511Z",
     "shell.execute_reply": "2025-05-30T10:07:33.050293Z"
    },
    "papermill": {
     "duration": 1.675253,
     "end_time": "2025-05-30T10:07:33.053171",
     "exception": true,
     "start_time": "2025-05-30T10:07:31.377918",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading test data...\n",
      "Test data shape: (5, 6)\n",
      "Loading tokenizer...\n",
      "Loading tokenizer from local path: /kaggle/input/deberta-v3/deberta_base_cache\n",
      "✅ Tokenizer loaded successfully\n",
      "Starting inference...\n",
      "Processing fold 0...\n",
      "❌ Model file not found: /kaggle/input/nbme-deberta-v3/nbme_ckpt/fold0.pt\n",
      "Processing fold 1...\n",
      "❌ Model file not found: /kaggle/input/nbme-deberta-v3/nbme_ckpt/fold1.pt\n",
      "Processing fold 2...\n",
      "❌ Model file not found: /kaggle/input/nbme-deberta-v3/nbme_ckpt/fold2.pt\n",
      "Processing fold 3...\n",
      "❌ Model file not found: /kaggle/input/nbme-deberta-v3/nbme_ckpt/fold3.pt\n",
      "Processing fold 4...\n",
      "❌ Model file not found: /kaggle/input/nbme-deberta-v3/nbme_ckpt/fold4.pt\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "❌ No models were successfully loaded!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13/412120855.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_13/412120855.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"❌ No models were successfully loaded!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;31m# ====================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: ❌ No models were successfully loaded!"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# main\n",
    "# ====================================================\n",
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    LOGGER.info(f\"Using device: {device}\")\n",
    "    \n",
    "    # ====================================================\n",
    "    # Load test data\n",
    "    # ====================================================\n",
    "    LOGGER.info(\"Loading test data...\")\n",
    "    test = pd.read_csv('/kaggle/input/nbme-score-clinical-patient-notes/test.csv')\n",
    "    features = pd.read_csv('/kaggle/input/nbme-score-clinical-patient-notes/features.csv')\n",
    "    patient_notes = pd.read_csv('/kaggle/input/nbme-score-clinical-patient-notes/patient_notes.csv')\n",
    "    \n",
    "    test = test.merge(features, on=['feature_num', 'case_num'], how='left')\n",
    "    test = test.merge(patient_notes, on=['pn_num', 'case_num'], how='left')\n",
    "    LOGGER.info(f\"Test data shape: {test.shape}\")\n",
    "\n",
    "    # ====================================================\n",
    "    # Load tokenizer\n",
    "    # ====================================================\n",
    "    LOGGER.info(\"Loading tokenizer...\")\n",
    "    try:\n",
    "        if os.path.exists(CFG.model):\n",
    "            LOGGER.info(f\"Loading tokenizer from local path: {CFG.model}\")\n",
    "            tokenizer = AutoTokenizer.from_pretrained(CFG.model)\n",
    "        else:\n",
    "            LOGGER.info(\"Local path not found, using online model...\")\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
    "            CFG.model = \"microsoft/deberta-base\"\n",
    "        LOGGER.info(\"✅ Tokenizer loaded successfully\")\n",
    "    except Exception as e:\n",
    "        LOGGER.error(f\"❌ Failed to load tokenizer: {e}\")\n",
    "        raise\n",
    "\n",
    "    # ====================================================\n",
    "    # Create dataset and dataloader\n",
    "    # ====================================================\n",
    "    test_dataset = TestDataset(tokenizer, test)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=CFG.num_workers,\n",
    "        pin_memory=True, \n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    # ====================================================\n",
    "    # Model inference\n",
    "    # ====================================================\n",
    "    predictions = []\n",
    "    LOGGER.info(\"Starting inference...\")\n",
    "    \n",
    "    for fold in CFG.trn_fold:\n",
    "        LOGGER.info(f\"Processing fold {fold}...\")\n",
    "        \n",
    "        try:\n",
    "            model_path = f'/kaggle/input/nbme-deberta-v3/nbme_ckpt/fold{fold}.pt'\n",
    "            if not os.path.exists(model_path):\n",
    "                LOGGER.error(f\"❌ Model file not found: {model_path}\")\n",
    "                continue\n",
    "                \n",
    "            # 載入模型\n",
    "            model = DebertaForTokenBinary(CFG.model)\n",
    "            checkpoint = torch.load(model_path, map_location='cpu')\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            LOGGER.info(f\"✅ Model loaded for fold {fold}\")\n",
    "            \n",
    "            # 推論\n",
    "            prediction = inference_fn(test_loader, model, device)\n",
    "            predictions.append(prediction)\n",
    "            LOGGER.info(f\"✅ Fold {fold} inference completed\")\n",
    "            \n",
    "            del model, checkpoint, prediction\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        except Exception as e:\n",
    "            LOGGER.error(f\"❌ Error in fold {fold}: {e}\")\n",
    "            import traceback\n",
    "            LOGGER.error(traceback.format_exc())\n",
    "            continue\n",
    "\n",
    "    if len(predictions) == 0:\n",
    "        raise RuntimeError(\"❌ No models were successfully loaded!\")\n",
    "\n",
    "    # ====================================================\n",
    "    # Ensemble and post-processing\n",
    "    # ====================================================\n",
    "    LOGGER.info(\"Averaging predictions...\")\n",
    "    predictions = np.mean(predictions, axis=0)\n",
    "\n",
    "    LOGGER.info(\"Post processing...\")\n",
    "    char_probs = get_char_probs(test['pn_history'].values, predictions, tokenizer)\n",
    "    \n",
    "    # 🔥 新增：閾值搜索\n",
    "    best_threshold = find_optimal_threshold(char_probs, ground_truth_locations=None)\n",
    "    \n",
    "    predictions = get_predictions_from_char_probs(char_probs, threshold=best_threshold)\n",
    "\n",
    "    # ====================================================\n",
    "    # Create submission\n",
    "    # ====================================================\n",
    "    LOGGER.info(\"Creating submission...\")\n",
    "    test['location'] = predictions\n",
    "    submission = test[['id', 'location']].copy()\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "    LOGGER.info(\"✅ submission.csv saved\")\n",
    "    LOGGER.info(f\"Submission shape: {submission.shape}\")\n",
    "    print(submission.head())\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 3075283,
     "isSourceIdPinned": false,
     "sourceId": 33607,
     "sourceType": "competition"
    },
    {
     "datasetId": 7552460,
     "sourceId": 12005462,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 25568107,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 242336254,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 32.276098,
   "end_time": "2025-05-30T10:07:36.331316",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-30T10:07:04.055218",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
